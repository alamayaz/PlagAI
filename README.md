# AI-Powered Plagiarism Detection System

A comprehensive plagiarism detection system that combines multiple similarity algorithms, web search integration, and AI-powered suggestions to analyze text documents for potential plagiarism. Built with Python using a single-agent LangGraph workflow architecture and featuring a modern web interface.

## Overview

This system provides multi-layered plagiarism detection through:
- **Statistical Analysis**: Jaccard similarity and TF-IDF cosine similarity
- **Semantic Analysis**: Sentence embeddings for contextual similarity
- **Web Verification**: Google Custom Search integration for online source checking
- **AI Enhancement**: Integrated OpenAI API for content improvement
- **Automated Workflow**: Single LangGraph agent with state management for reliable processing

## Architecture

### Agent-Based Design
The system uses **1 primary LangGraph agent** (`PlagiarismDetectionAgent`) that manages the entire workflow through state transitions and conditional routing. This single-agent architecture ensures consistent state management and simplified error handling across the entire pipeline.

### Workflow DAG (Directed Acyclic Graph)
![Plagiarism Detection Workflow](PlagGraphDAG).png)

*Place the workflow diagram image as `workflow_dag.png` in your repository root directory*

The workflow follows a directed acyclic graph pattern with the following processing stages:

1. **parse_input**: Entry point that validates and categorizes input data
2. **extract_text**: File processing for PDF, DOCX, and TXT formats  
3. **preprocess_text**: Text cleaning, tokenization, and chunking
4. **detect_plagiarism**: Core similarity analysis using multiple algorithms
5. **analyze_results**: Results processing and metric extraction
6. **generate_suggestions**: AI-powered improvement recommendations (conditional based on plagiarism score)
7. **create_report**: Final report generation and formatting
8. **handle_error**: Error handling and recovery with multiple entry points

The diagram shows the complete workflow with:
- **Solid arrows**: Direct sequential flow
- **Dotted arrows**: Conditional routing based on decision points
- **3 decision points**: File type routing, error handling, and suggestion generation threshold

### Core Components

**Detection Engine** (`plagiarism_detector.py`)
- Multiple similarity algorithms (Jaccard, TF-IDF, Semantic)
- Text preprocessing and chunking
- Web search integration
- OpenAI API integration for suggestions

**Single Agent Workflow** (`langgraph_plagiarism_agent.py`)
- LangGraph-based state management with single agent
- Conditional routing and error handling
- File format support (PDF, DOCX, TXT)
- Batch processing capabilities

**Web Interface** (`streamlit_plagiarism_app.py`)
- Interactive Streamlit dashboard
- Real-time analysis visualization
- Export functionality
- Progress tracking

### Agent Processing Pipeline

```
Input Text/File â†’ Parse & Validate â†’ Extract Text (if file) â†’ 
Preprocess & Clean â†’ Detect Plagiarism â†’ Analyze Results â†’ 
Generate AI Suggestions (if needed) â†’ Create Final Report
```

The single agent handles all state transitions, conditional branching, and error recovery through the LangGraph framework.

## ğŸš€ Features

### Detection Methods
- **Jaccard Similarity**: Word overlap analysis between texts
- **TF-IDF Cosine Similarity**: Document vector comparison
- **Semantic Similarity**: Context-aware similarity using sentence transformers
- **Web Search Verification**: Compare against online sources via Google API

### File Support
- **PDF Files**: Automatic text extraction using PyPDF2
- **Word Documents**: DOCX file processing with python-docx
- **Plain Text**: Direct text file reading
- **Raw Text Input**: Direct text paste functionality

### AI Features
- **Smart Suggestions**: OpenAI-powered rewriting recommendations
- **Context Awareness**: Suggestions tailored to specific flagged content
- **Multiple Options**: Various paraphrasing and restructuring approaches

### User Interface
- **Real-time Dashboard**: Interactive web interface with live updates
- **Visual Analytics**: Plotly charts showing similarity scores and source distribution
- **Batch Processing**: Analyze multiple documents simultaneously
- **Export Options**: JSON, CSV, and formatted text reports

## ğŸ“‹ Requirements

### System Requirements
- Python 3.8 or higher
- Internet connection (for web search and AI features)
- 2GB RAM minimum (4GB recommended)

### Dependencies
```
streamlit>=1.28.0
plotly>=5.15.0
pandas>=2.0.0
openai>=1.3.0
sentence-transformers>=2.2.0
scikit-learn>=1.3.0
nltk>=3.8.0
langgraph>=0.0.40
PyPDF2>=3.0.0
python-docx>=0.8.11
requests>=2.31.0
python-dotenv>=1.0.0
```

## âš™ï¸ Installation

### 1. Clone Repository
```bash
git clone <repository-url>
cd plagiarism-detection-system
```

### 2. Create Virtual Environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Download NLTK Data
```bash
python -c "import nltk; nltk.download('punkt'); nltk.download('punkt_tab'); nltk.download('stopwords')"
```

### 5. Configure API Keys
Create a `.env` file in the project root:
```env
# Required: OpenAI API Key
OPENAI_API_KEY=your_openai_api_key_here

# Optional: Google Custom Search (recommended for enhanced detection)
GOOGLE_API_KEY=your_google_api_key_here
GOOGLE_CSE_ID=your_custom_search_engine_id_here
```

#### Getting API Keys

**OpenAI API Key:**
1. Visit [OpenAI API Dashboard](https://platform.openai.com/api-keys)
2. Create account and generate API key
3. Add billing information for usage

**Google Custom Search (Optional):**
1. Go to [Google Cloud Console](https://console.cloud.google.com/)
2. Enable "Custom Search JSON API"
3. Create API credentials
4. Set up Custom Search Engine at [cse.google.com](https://cse.google.com/)
5. Configure to search entire web (`*` in sites to search)

## ğŸƒ Usage

### Web Interface (Recommended)
```bash
streamlit run streamlit_plagiarism_app.py
```
Access the application at `http://localhost:8501`

### Command Line Interface
```bash
# Analyze text directly
python langgraph_plagiarism_agent.py --text "Your text here" --openai-key "your_key"

# Analyze a file
python langgraph_plagiarism_agent.py --file "document.pdf" --openai-key "your_key"

# Batch processing
python langgraph_plagiarism_agent.py --batch "input_files.json" --openai-key "your_key"
```

### Python API
```python
from plagiarism_detector import PlagiarismDetector

# Initialize detector
detector = PlagiarismDetector(
    openai_api_key="your_openai_key",
    google_api_key="your_google_key",  # Optional
    google_cse_id="your_cse_id"        # Optional
)

# Analyze text
results = detector.analyze_text("Your text to analyze")
print(f"Plagiarism: {results['overall_plagiarism_percentage']:.1f}%")

# Generate report
report = detector.format_report(results)
print(report)
```

## ğŸ“Š Understanding Results

### Plagiarism Scores
- **0-15%**: Low risk (likely original content)
- **15-30%**: Medium risk (requires review)
- **30%+**: High risk (significant similarities found)

### Similarity Methods
- **Jaccard**: Measures word overlap between texts
- **TF-IDF**: Compares document term frequency patterns
- **Semantic**: Analyzes contextual meaning similarity

### Output Formats
- **JSON**: Complete analysis data with metadata
- **CSV**: Tabular chunk-by-chunk analysis
- **Text Report**: Human-readable formatted summary

## ğŸ”§ Configuration

### Detection Settings
Edit configuration in the web interface sidebar:
- **Chunk Size**: Number of sentences per analysis chunk (2-6)
- **Web Search**: Enable/disable online source checking
- **Similarity Thresholds**: Adjust sensitivity levels
- **Detection Mode**: Standard vs Enhanced analysis

### Advanced Settings
Modify thresholds in `plagiarism_detector.py`:
```python
PLAGIARISM_THRESHOLD = 0.5  # Consider as plagiarized
WEB_SEARCH_THRESHOLD = 0.3  # Include in web search results
SUGGESTION_THRESHOLD = 20   # Generate suggestions above this percentage
```

## ğŸ¨ Workflow Visualization

Generate workflow diagrams:
```bash
python graph.py
```

This creates:
- Mermaid diagram code (for web visualization)
- ASCII art workflow chart
- Processing pipeline overview

## ğŸ§ª Testing

### Run Test Suite
```bash
# Test installation
python test_plagiarism.py

# Test OpenAI integration
python debug_suggestions.py

# Test web interface
python test_streamlit.py
```

### Sample Analysis
```bash
# Quick test with sample text
python simple_example.py
```

## ğŸ“ Project Structure

```
plagiarism-detection-system/
â”œâ”€â”€ plagiarism_detector.py           # Core detection engine
â”œâ”€â”€ enhanced_plagiarism_detector.py  # Extended detection methods
â”œâ”€â”€ langgraph_plagiarism_agent.py    # LangGraph workflow agent
â”œâ”€â”€ streamlit_plagiarism_app.py      # Web interface
â”œâ”€â”€ graph.py                         # Workflow visualization
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ .env                            # API keys (create this)
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ test_scripts/
â”‚   â”œâ”€â”€ test_plagiarism.py          # Installation tests
â”‚   â”œâ”€â”€ debug_suggestions.py        # OpenAI debugging
â”‚   â””â”€â”€ simple_example.py           # Quick start example
â””â”€â”€ docs/
    â”œâ”€â”€ setup_guide.md              # Detailed setup instructions
    â””â”€â”€ api_documentation.md        # API reference
```

## ğŸ” Troubleshooting

### Common Issues

**Import Errors:**
```bash
pip install --upgrade -r requirements.txt
```

**NLTK Data Missing:**
```bash
python -c "import nltk; nltk.download('all')"
```

**OpenAI API Errors:**
- Verify API key is correct
- Check billing status in OpenAI dashboard
- Monitor rate limits and usage

**Google Search Errors:**
- Ensure Custom Search JSON API is enabled
- Verify CSE ID format (should be alphanumeric with colon, not email)
- Check API quotas in Google Cloud Console

**Streamlit Issues:**
```bash
streamlit --version
pip install --upgrade streamlit
```

### Performance Tips

**For Faster Analysis:**
- Disable web search for local-only checking
- Use smaller chunk sizes for shorter texts
- Limit batch processing to 10 documents at once

**For Better Accuracy:**
- Enable all detection methods
- Use both Google and OpenAI integrations
- Increase chunk overlap for better coverage

## Performance Metrics

### Single Agent Architecture Benefits
- **State Consistency**: Single agent ensures reliable state management across all processing steps
- **Error Recovery**: Centralized error handling with multiple recovery paths shown in DAG
- **Memory Efficiency**: Single agent instance handles entire workflow without inter-agent communication overhead

### Processing Performance
- **Processing Speed**: ~1-2 seconds per 1000 words (local algorithms)
- **Web Search**: +2-3 seconds per chunk (depends on network)
- **AI Suggestions**: +3-5 seconds (depends on OpenAI response time)
- **Memory Usage**: ~200MB base + ~50MB per document
- **Accuracy**: 85-95% detection rate for academic text similarities
- **Agent Overhead**: Minimal due to single-agent design (~10-20ms state transitions)

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/improvement`)
3. Commit changes (`git commit -am 'Add feature'`)
4. Push to branch (`git push origin feature/improvement`)
5. Create Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™‹ Support

For issues, questions, or contributions:
- Create an issue on GitHub
- Check the troubleshooting section above
- Review the setup guide in `/docs/setup_guide.md`

## ğŸ¯ Future Enhancements

- [ ] Support for more file formats (RTF, ODT)
- [ ] Integration with academic databases (ArXiv, PubMed)
- [ ] Advanced citation detection
- [ ] Multi-language support
- [ ] REST API for integration
- [ ] Docker containerization
- [ ] Cloud deployment options

---

**Built with â¤ï¸ using Python, OpenAI, LangGraph, and Streamlit**